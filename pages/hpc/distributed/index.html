<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/ws23_ulg_vu_ama_performance/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/ws23_ulg_vu_ama_performance/libs/highlight/github.min.css">
   
    <script src="/ws23_ulg_vu_ama_performance/libs/clipboard.min.js"></script>
  
  
  <script src="/ws23_ulg_vu_ama_performance/libs/plotly-1_58_5.min.js"></script> 
  <script>
    // This function is used when calling `\fig{...}` See # Using \fig{...} below
    const PlotlyJS_json = async (div, url) => {
      response = await fetch(url); // get file
      fig = await response.json(); // convert it to json
      // Make the plot fit the screen responsively. See the documentation of plotly.js. https://plotly.com/javascript/responsive-fluid-layout/
      if (typeof fig.config === 'undefined') { fig["config"]={} }
      delete fig.layout.width
      delete fig.layout.height
      fig["layout"]["autosize"] = true
      fig["config"]["autosizable"] = true
      fig["config"]["responsive"] = true

      // make it easier to scroll throught the website rather than being blocked by a figure.
      fig.config["scrollZoom"] = false

      // PlotlyJS.savefig by default add the some more attribute to make a static plot.
      // Disable them to make the website fancier.
      delete fig.config.staticPlot
      delete fig.config.displayModeBar
      delete fig.config.doubleClick
      delete fig.config.showTips

      Plotly.newPlot(div, fig);
    };
  </script>
  
  <link rel="stylesheet" href="/ws23_ulg_vu_ama_performance/css/jtd.css">
<link rel="stylesheet" href="/ws23_ulg_vu_ama_performance/css/extras.css">
<link rel="icon" href="/ws23_ulg_vu_ama_performance/assets/favicon.ico">

<style>
  /* #148 wrap long header */
  .franklin-content a.header-anchor,
  .franklin-toc li a
   {
    word-wrap: break-word;
    white-space: normal;
  }
</style>

   <title>Parallel Computing - Distributed Computing</title>  
</head>
<body>                      <!-- closed in foot.html -->
<div class="page-wrap">   <!-- closed in foot.html -->
  <!-- SIDE BAR -->
  <div class="side-bar">
    <div class="header">
      <a href="/ws23_ulg_vu_ama_performance/" class="title">
        Julia
      </a>
    </div>
    <label for="show-menu" class="show-menu">MENU</label>
    <input type="checkbox" id="show-menu" role="button">
    <div class="menu" id="side-menu">
      <ul class="menu-list">
        <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/" class="menu-list-link ">Start</a>
        <li class="menu-list-item active"><a href="/ws23_ulg_vu_ama_performance/pages/hpc/" class="menu-list-link active">Parallel computing</a>
          <ul class="menu-list-child-list ">
            <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/hpc/performance/" class="menu-list-link ">Measuring performance</a>
            <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/hpc/simd/" class="menu-list-link ">Single Instruction Multiple Data</a>
            <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/hpc/pi/" class="menu-list-link ">&pi; example</a>
            <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/hpc/multithreading/" class="menu-list-link ">Multithreading</a>
            <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/hpc/distributed/" class="menu-list-link active">Distributed computing</a>
            <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/hpc/gpu/" class="menu-list-link ">GPU computing</a>
          </ul>
        <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/" class="menu-list-link ">Remote computing</a>
          <ul class="menu-list-child-list ">
            <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/history" class="menu-list-link ">History</a>
            <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/hpc/" class="menu-list-link ">High Performance Computing</a>
            <ul class="menu-list-child-list ">
              <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/hpc/basics" class="menu-list-link ">Basics</a>
              <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/hpc/architecture" class="menu-list-link ">Architecture</a>
              <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/hpc/usage" class="menu-list-link ">Usage</a>
            </ul>
            <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/cloud/" class="menu-list-link ">Cloud Computing</a>
              <ul class="menu-list-child-list ">
                <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/cloud/basics" class="menu-list-link ">Basics</a>
                <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/cloud/architecture" class="menu-list-link ">Architecture</a>
                <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/cloud/setup_vhpc" class="menu-list-link ">Setup vHPC</a>
              </ul>
            <li class="menu-list-item "><a href="/ws23_ulg_vu_ama_performance/pages/remote-computing/notes/" class="menu-list-link ">Notes</a>
          </ul>
          <li class="menu-list-item active"><a href="/ws23_ulg_vu_ama_performance/pages/exercises/" class="menu-list-link ">Exercises</a>
      </ul>
    </div>
    <div class="footer">
      This is <em>Just the docs</em>, adapted from the <a href="https://github.com/pmarsceill/just-the-docs" target="_blank">Jekyll theme</a>.
    </div>
  </div>
  <!-- CONTENT -->
  <div class="main-content-wrap"> <!-- closed in foot.html -->
    <div class="main-content">    <!-- closed in foot.html -->
      <div class="main-header">
        WS23 971007 VU Advanced Methods and its Applications - Performance topics
      </div>



<!-- Content appended here (in class franklin-content) -->
<div class="franklin-content"><h1 id="distributed_computing_in_julia"><a href="#distributed_computing_in_julia" class="header-anchor">Distributed computing in Julia</a></h1>
<div class="franklin-toc"><ol><li><a href="#tasks">Tasks</a><ol><li><a href="#the_pi_example_with_tasks">The <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> example with tasks</a></li></ol></li><li><a href="#back_to_distributed_computing">Back to distributed computing</a><ol><li><a href="#the_pi_example_for_distributed_computing">The <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> example for distributed computing</a></li><li><a href="#the_distributed_macro">The <code>@distributed</code> macro</a></li><li><a href="#the_pmap_and_the_everywhere_macro">The <code>pmap</code> and the <code>@everywhere</code> macro</a></li><li><a href="#final_results">Final results</a></li></ol></li></ol></div>
<p>In Julia we talk about distributed computing when we run Julia processes with separate memory spaces. Julia does not distinguish between processes on the same or on multiple computers. The main idea is to have remote execution.</p>
<p>Before we start with distributed computing we introduce tasks.</p>
<h2 id="tasks"><a href="#tasks" class="header-anchor">Tasks</a></h2>
<p>Tasks are part of the <a href="https://docs.julialang.org/en/v1/manual/asynchronous-programming/">asynchronous programming</a> concepts implemented in Julia. We can think of a task as a work package with a create-start-run-finish life cycle. This means, a task can be created and scheduled independently. Tasks are the basic building block for performing distributed computing in Julia. This concept of tasks is actually rather similar to how High Performance Computing &#40;HPC&#41; systems work with their job scheduler.</p>
<p>To create a task we can use the <code>@task</code> macro, that will return a <em>runnable</em> but will not execute it. We use the <code>schedule</code> command to actually execute it.</p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> t = <span class="hljs-meta">@task</span> <span class="hljs-keyword">begin</span>; sleep(<span class="hljs-number">5</span>); println(<span class="hljs-string">&quot;done&quot;</span>); <span class="hljs-keyword">end</span>
</span>Task (runnable) @0x00007fb859f336b0

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> schedule(t)
</span>Task (runnable) @0x00007fb859f336b0

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> done</span></code></pre>
<p>The main idea is, that a task can be interrupted and the order of tasks is not set. This means if we have multiple tasks the order of execution is not guaranteed. This also means, that the main Julia process is not blocked by a task but works independently. If we want to wait for the task to finish before the calling task continues, we can do this with <code>wait&#40;t&#41;</code>.</p>
<p>Most of the time we create a task and schedule it right away. This can be done by the <code>@async</code> macro. It is basically equivalent to <code>schedule&#40;@task x&#41;</code>. We can also catch the task &#40;and therefore the state&#41; with <code>t &#61; @async x</code>. If we want to wait for several tasks, we can use the <code>@sync</code> macro. It will wait until all enclosed tasks spanned by <code>@async</code>, <code>@spawn</code>, <code>@spawnat</code>, and <code>@distributed</code> are completed.</p>
<p>A task can also return a value, we can get the value with <code>fetch&#40;t&#41;</code> or with <code>take&#33;&#40;t&#41;</code>,  which will remove the value as well.</p>
<button type="button" class="collapsible" style="background-color:#caffa5"> Example </button><div class="collapsiblecontent">  The following examples are taken from <a href="https://juliaacademy.com/p/parallel-computing">Parallel Computing Class on JuliaAcademy.com</a>, section <em>Tasks</em>:</p>
<ol>
<li><p>How long will this take?</p>
</li>
</ol>
<pre><code class="julia hljs"><span class="hljs-meta">@time</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:<span class="hljs-number">10</span>
          sleep(<span class="hljs-number">1</span>)
      <span class="hljs-keyword">end</span></code></pre>
<ol start="2">
<li><p>How long will this take?</p>
</li>
</ol>
<pre><code class="julia hljs"><span class="hljs-meta">@time</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:<span class="hljs-number">10</span>
          <span class="hljs-meta">@async</span> sleep(<span class="hljs-number">1</span>)
      <span class="hljs-keyword">end</span></code></pre>
<ol start="3">
<li><p>How long will this take?</p>
</li>
</ol>
<pre><code class="julia hljs"><span class="hljs-meta">@time</span> <span class="hljs-meta">@sync</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:<span class="hljs-number">10</span>
                <span class="hljs-meta">@async</span> sleep(<span class="hljs-number">1</span>)
            <span class="hljs-keyword">end</span></code></pre>
<p><div class="solution"> Solution </div><div class="solutioncollapsible">  </p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@time</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:<span class="hljs-number">10</span>
                 sleep(<span class="hljs-number">1</span>)
             <span class="hljs-keyword">end</span>
</span> 10.020172 seconds (51 allocations: 1.703 KiB)

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@time</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:<span class="hljs-number">10</span>
                 <span class="hljs-meta">@async</span> sleep(<span class="hljs-number">1</span>)
             <span class="hljs-keyword">end</span>
</span>  0.017384 seconds (6.40 k allocations: 399.856 KiB, 96.91% compilation time)

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@time</span> <span class="hljs-meta">@sync</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:<span class="hljs-number">10</span>
                       <span class="hljs-meta">@async</span> sleep(<span class="hljs-number">1</span>)
                   <span class="hljs-keyword">end</span>
</span>  1.047838 seconds (847 allocations: 53.656 KiB, 4.34% compilation time)</code></pre>
<p></div> </div>
<h3 id="the_pi_example_with_tasks"><a href="#the_pi_example_with_tasks" class="header-anchor">The <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> example with tasks</a></h3>
<p>Now we apply the task knowledge to our example for computing <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span>. In this chapter we will use the following stub throughout the exercises:</p>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> Base.Threads
<span class="hljs-keyword">using</span> BenchmarkTools
<span class="hljs-keyword">using</span> Distributed
<span class="hljs-keyword">using</span> Random


<span class="hljs-keyword">function</span> sample_M_non_distributed_rng(N::<span class="hljs-built_in">Int64</span>, rng::AbstractRNG)
    M = zero(<span class="hljs-built_in">Int64</span>)

    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:N
        <span class="hljs-keyword">if</span> (rand(rng)^<span class="hljs-number">2</span> + rand(rng)^<span class="hljs-number">2</span>) &lt; <span class="hljs-number">1</span>
            M += <span class="hljs-number">1</span>
        <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">return</span> M
<span class="hljs-keyword">end</span>


<span class="hljs-keyword">function</span> estimate_pi_distributed(N::<span class="hljs-built_in">Int64</span>, method::<span class="hljs-built_in">Symbol</span>)
    <span class="hljs-keyword">function</span> sample_M_non_distributed(N::<span class="hljs-built_in">Int64</span>)
        <span class="hljs-keyword">return</span> sample_M_non_distributed_rng(N, Random.default_rng())
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">function</span> sample_M_tasks(N::<span class="hljs-built_in">Int64</span>)
        <span class="hljs-comment"># Exercise 1</span>
        <span class="hljs-keyword">return</span> <span class="hljs-literal">nothing</span>
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">function</span> sample_M_distributed(N::<span class="hljs-built_in">Int64</span>)
        <span class="hljs-comment"># Exercise 2</span>
        <span class="hljs-keyword">return</span> <span class="hljs-literal">nothing</span>
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">function</span> sample_M_distributed_pmap(N::<span class="hljs-built_in">Int64</span>)
        <span class="hljs-comment"># Exercise 3</span>
        <span class="hljs-keyword">function</span> sample_M_distributed_pmap_inner(N::<span class="hljs-built_in">Int64</span>)
            M = zero(eltype(N))

            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:N
                <span class="hljs-keyword">if</span> (rand()^<span class="hljs-number">2</span> + rand()^<span class="hljs-number">2</span>) &lt; <span class="hljs-number">1</span>
                    M += <span class="hljs-number">1</span>
                <span class="hljs-keyword">end</span>
            <span class="hljs-keyword">end</span>

            <span class="hljs-keyword">return</span> M
        <span class="hljs-keyword">end</span>

        <span class="hljs-keyword">return</span> <span class="hljs-literal">nothing</span>
    <span class="hljs-keyword">end</span>

     function_mapping = <span class="hljs-built_in">Dict</span>(
         :ex0 =&gt; sample_M_non_distributed,
         :ex1 =&gt; sample_M_tasks,
         :ex2 =&gt; sample_M_distributed,
         :ex3 =&gt; sample_M_distributed_pmap,
     )

     M = function_mapping[method](N)

     est_pi = <span class="hljs-number">4</span> * M / N

    <span class="hljs-keyword">return</span> est_pi, abs(<span class="hljs-literal">pi</span> - est_pi)
<span class="hljs-keyword">end</span></code></pre>
<button type="button" class="collapsible" style="background-color:#b5ddff"> Exercise </button><div class="collapsiblecontent">  Using tasks:</p>
<ol>
<li><p>Complete the function <code>sample_M_tasks</code> by using the <code>@async</code> and <code>@sync</code> macros.</p>
</li>
<li><p>Split the generation of the <code>N</code> samples into 4 parts, same as for threads, and schedule the tasks in a loop.</p>
</li>
<li><p>Test the result as well as the performance.</p>
</li>
</ol>
<p>Hints: Define a vector of tasks <code>t &#61; Vector&#123;Task&#125;&#40;undef, n&#41;</code> to catch the results inside the loop. You can use <code>sample_M_non_distributed&#40;&#41;</code> for sampling.</p>
<p><div class="solution"> Solution </div><div class="solutioncollapsible">  Have a look at the end of this chapter. </div> </div>
<h2 id="back_to_distributed_computing"><a href="#back_to_distributed_computing" class="header-anchor">Back to distributed computing</a></h2>
<p>Now that we know what a task is and how to create one we have no difficulty to define what distributed computing is. It is simply the way to distribute tasks on multiple CPUs or computers. In Julia, this multiprocessing environment is based on message passing. It allows tasks to run on multiple processes in separate memory domains, but all at once. The communication in Julia is not like the one used by <a href="https://www.mpi-forum.org/docs/">MPI</a>. It is <em>one-sided</em>, that is we only need to manage one process in a two-process operation. These management instructions are also not sent/receive messages but calls to functions or something similar. For this, Julia provides two primitives, <em>remote reference</em> and <em>remote calls</em>, the <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/">documentation</a> tells us: </p>
<blockquote>
<p>A remote reference is an object that can be used from any process to refer to an object stored on a particular process. A remote call is a request by one process to call a certain function on certain arguments on another &#40;possibly the same&#41; process.</p>
</blockquote>
<p>All of this is managed by the <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/#man-distributed"><code>Distributed</code></a> package. It is easy to imagine, that the field of distributed computing can quickly become quite extensive, so let us look at some concepts that are useful for our <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> example and give us an idea on how this works.</p>
<h3 id="the_pi_example_for_distributed_computing"><a href="#the_pi_example_for_distributed_computing" class="header-anchor">The <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> example for distributed computing</a></h3>
<p>In order to start with distributed computing, we need to add some distributed processes or <em>workers</em>. Similar as with threads each process has an associated identifier. The process providing the Julia REPL or the main call has id <code>1</code>. As long as there are more than two processes, each process that does not have id <code>1</code> is considered a worker process.</p>
<p>We can add workers at startup:</p>
<pre><code class="bash hljs">$ julia -p 2</code></pre>
<p>where we defined 2 workers on the local machine. We can also add &#40;further&#41; workers from within Julia by calling </p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-keyword">using</span> Distributed
</span>
<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> addprocs(<span class="hljs-number">4</span>)
</span>4-element Vector{Int64}:
 4
 5
 6
 7</code></pre>
<p>This added four workers and in total we have now seven processes. The return value gives the IDs of the recently added workers. You can also use <code>procs&#40;&#41;</code> to get a full list of active processes. Consequently, we can remove workers again by calling</p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> rmprocs(<span class="hljs-number">2</span>:<span class="hljs-number">5</span>)
</span>Task (done) @0x00007fb859fc1430</code></pre>
<p>and with <code>myid&#40;&#41;</code> we get the id of the process we are on.</p>
<p>Note that we will often need the same packages on the worker nodes as on the main node. Therefor we need to call julia with the <code>--project</code> flag,</p>
<pre><code class="julia hljs">$ julia -p <span class="hljs-number">4</span> --project</code></pre>
<p>or tell so when adding workers:</p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-keyword">using</span> Distributed
</span>
<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> addprocs(<span class="hljs-number">4</span>, exeflags=<span class="hljs-string">&quot;--process&quot;</span>)</span></code></pre>
<h3 id="the_distributed_macro"><a href="#the_distributed_macro" class="header-anchor">The <code>@distributed</code> macro</a></h3>
<p>There is an obvious problem right away. If we define a function or a variable on a process how does another process know about this? </p>
<p>The easiest concept is to run a parallel <code>for</code> loop. As before we can do this with a macro, namely the <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.@distributed"><code>@distributed</code></a> macro. The general construct is</p>
<pre><code class="julia hljs"><span class="hljs-meta">@distributed</span> [reducer] <span class="hljs-keyword">for</span> var = range
    body
<span class="hljs-keyword">end</span></code></pre>
<blockquote>
<p>The specified range is partitioned and locally executed across all workers. In case an optional reducer function is specified, <code>@distributed</code> performs local reductions on each worker with a final reduction on the calling process.</p>
</blockquote>
<blockquote>
<p>Note that without a reducer function, @distributed executes asynchronously, i.e. it spawns independent tasks on all available workers and returns immediately without waiting for completion. To wait for completion, prefix the call with @sync, like :</p>
</blockquote>
<pre><code class="julia hljs"><span class="hljs-meta">@sync</span> <span class="hljs-meta">@distributed</span> <span class="hljs-keyword">for</span> var = range
    body
<span class="hljs-keyword">end</span></code></pre>
<button type="button" class="collapsible" style="background-color:#caffa5"> Example </button><div class="collapsiblecontent">  What is the result for the following code snippet?</p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> workers()
</span>4-element Vector{Int64}:
 2
 3
 4 
 5

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> a = zeros(<span class="hljs-number">5</span>);
</span>
<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@sync</span> <span class="hljs-meta">@distributed</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:<span class="hljs-number">5</span>
                              a[i] = i
                          <span class="hljs-keyword">end</span></span></code></pre>
<p></div>
<p>Luckily, our <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> example does not need much data movement.</p>
<button type="button" class="collapsible" style="background-color:#b5ddff"> Exercise </button><div class="collapsiblecontent">  Using distribute:</p>
<ol>
<li><p>Complete the function <code>sample_M_distributed</code> by using the <code>@distributed</code> macro and an appropriate <em>reducer</em> function &#40;the syntax is <code>x &#61; @distributed &#40;operator&#41; for ...</code>&#41;.</p>
</li>
<li><p>Start Julia with 4 workers &#40;<code>julia -p 4 --project</code>&#41;, test the accuracy and measure the performance.</p>
</li>
</ol>
<p><div class="solution"> Solution </div><div class="solutioncollapsible">  Have a look at the end of this chapter. </div> </div>
<p>We can see that this is a very easy way to parallelize and this time the <code>rand&#40;&#41;</code> function is not causing problems since it is local per task. We are already faster than the basic implementation and close to the optimized four threads implementation.</p>
<p>The <em>distributed for loop</em> with <code>@distributed</code> is designed to work well for situations where each iteration is tiny &#40;in terms of computational effort/workload&#41;. Of course, there is also the other possibility, that we have a function with a massive workload and work with the results of these calls.</p>
<h3 id="the_pmap_and_the_everywhere_macro"><a href="#the_pmap_and_the_everywhere_macro" class="header-anchor">The <code>pmap</code> and the <code>@everywhere</code> macro</a></h3>
<p>As mentioned before, we need to get functions to all the workers, in order to execute them. For distributing a function or for loading modules we can use the <code>@everywhere</code> macro. As the name suggests, it will make sure that the function is available in the scope of each worker and the main process. We simply prepend a function with <code>@everywhere</code> and nothing more is required. There are some things to note for this case:</p>
<ul>
<li><p>the function will be compiled on each worker on the first call.</p>
</li>
<li><p>no local variables are captured but they can be broadcasted &#40;arguments are broadcasted&#41;</p>
</li>
</ul>
<pre><code class="julia hljs">foo = <span class="hljs-number">2</span>
<span class="hljs-meta">@everywhere</span> bar = $foo</code></pre>
<ul>
<li><p>the function will only be available on workers that were present during the call, every worker that is added later will not have it defined.</p>
</li>
<li><p>a module can be loaded on every worker with <code>@everywhere using &lt;modulename&gt;</code>.</p>
</li>
</ul>
<p>The function <a href="https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.pmap"><code>pmap</code></a> is the parallel version of the <a href="https://docs.julialang.org/en/v1/base/collections/#Base.map"><code>map</code></a> function. Let us start with <code>map</code> before we go on to <code>pmap</code>. The basic idea is to map a collection to a function by applying the function to each element.The result is again a collection. The syntax is as follows:</p>
<pre><code class="julia hljs">map(f, c...) -&gt; collection</code></pre>
<p>We can even include multiple collections and it will apply the function until one collection is exhausted.</p>
<button type="button" class="collapsible" style="background-color:#caffa5"> Example </button><div class="collapsiblecontent">  This examples is copied from the <a href="https://docs.julialang.org/en/v1/base/collections/#Base.map">docs</a></p>
<pre><code class="julia-repl hljs"><span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> map(+, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">400</span>, <span class="hljs-number">5000</span>])
</span>3-element Vector{Int64}:
 11
 22
 33

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> map(x -&gt; x * <span class="hljs-number">2</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])
</span>3-element Vector{Int64}:
 2
 4
 6</code></pre>
<p>The second example also includes an <a href="https://docs.julialang.org/en/v1/manual/functions/#man-anonymous-functions">anonymous function</a>. The idea is simply to have a function that is not needed outside of the scope of a function call. </div>
<p>Now, <code>pmap</code> just distributes the map function on workers. It has a lot of optional arguments to influence how this is done but we will not need this.</p>
<button type="button" class="collapsible" style="background-color:#b5ddff"> Exercise </button><div class="collapsiblecontent">  Using <code>pmap</code>:</p>
<ol>
<li><p>Complete the function <code>sample_M_distributed_pmap</code> by using the <code>@everywhere</code> macro and <code>pmap</code> function.</p>
</li>
<li><p>Start Julia with 4 workers &#40;<code>julia -p 4 --project</code>&#41;, test the accuracy and measure the performance.</p>
</li>
</ol>
<p>Hint: define an inner function <code>sample_M_distributed_pmap_inner</code> that is distributed to all workers via <code>@everywhere</code> and collect the results with <code>pmap</code>. Split up the workers similarly to the tasks example.</p>
<p><div class="solution"> Solution </div><div class="solutioncollapsible">  Have a look at the end of this chapter. </div> </div>
<h3 id="final_results"><a href="#final_results" class="header-anchor">Final results</a></h3>
<div class="solution"> Solution </div><div class="solutioncollapsible">  Here is the overall solution:</p>
<pre><code class="julia hljs"><span class="hljs-keyword">using</span> Base.Threads
<span class="hljs-keyword">using</span> BenchmarkTools
<span class="hljs-keyword">using</span> Distributed
<span class="hljs-keyword">using</span> Random


<span class="hljs-keyword">function</span> sample_M_non_distributed_rng(N::<span class="hljs-built_in">Int64</span>, rng::AbstractRNG)
    M = zero(<span class="hljs-built_in">Int64</span>)

    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:N
        <span class="hljs-keyword">if</span> (rand(rng)^<span class="hljs-number">2</span> + rand(rng)^<span class="hljs-number">2</span>) &lt; <span class="hljs-number">1</span>
            M += <span class="hljs-number">1</span>
        <span class="hljs-keyword">end</span>
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">return</span> M
<span class="hljs-keyword">end</span>


<span class="hljs-keyword">function</span> estimate_pi_distributed(N::<span class="hljs-built_in">Int64</span>, method::<span class="hljs-built_in">Symbol</span>)
    <span class="hljs-keyword">function</span> sample_M_non_distributed(N::<span class="hljs-built_in">Int64</span>)
        <span class="hljs-keyword">return</span> sample_M_non_distributed_rng(N, Random.default_rng())
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">function</span> sample_M_tasks(N::<span class="hljs-built_in">Int64</span>)
        n = nthreads()

        t = <span class="hljs-built_in">Vector</span>{<span class="hljs-built_in">Task</span>}(<span class="hljs-literal">undef</span>, n)
        len, _ = divrem(N, n)

        <span class="hljs-meta">@sync</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:n
            t[i] = <span class="hljs-meta">@async</span> sample_M_non_distributed(len)
        <span class="hljs-keyword">end</span>

        M = sum(fetch.(t))

        <span class="hljs-keyword">return</span> M
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">function</span> sample_M_distributed(N::<span class="hljs-built_in">Int64</span>)
        M = <span class="hljs-meta">@distributed</span> (+) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:N
            rand()^<span class="hljs-number">2</span> + rand()^<span class="hljs-number">2</span> &lt; <span class="hljs-number">1</span>
        <span class="hljs-keyword">end</span>

        <span class="hljs-keyword">return</span> M
    <span class="hljs-keyword">end</span>

    <span class="hljs-keyword">function</span> sample_M_distributed_pmap(N::<span class="hljs-built_in">Int64</span>)
        <span class="hljs-meta">@everywhere</span> <span class="hljs-keyword">function</span> sample_M_distributed_pmap_inner(N::<span class="hljs-built_in">Int64</span>)
            M = zero(eltype(N))

            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-number">1</span>:N
                <span class="hljs-keyword">if</span> (rand()^<span class="hljs-number">2</span> + rand()^<span class="hljs-number">2</span>) &lt; <span class="hljs-number">1</span>
                    M += <span class="hljs-number">1</span>
                <span class="hljs-keyword">end</span>
            <span class="hljs-keyword">end</span>

            <span class="hljs-keyword">return</span> M
        <span class="hljs-keyword">end</span>

        len, _ = divrem(N, nprocs() - <span class="hljs-number">1</span>)

        M = sum(
                pmap(
                        x -&gt; sample_M_distributed_pmap_inner(len),
                        <span class="hljs-number">2</span>:nprocs()
                )
        )

        <span class="hljs-keyword">return</span> M
    <span class="hljs-keyword">end</span>

     function_mapping = <span class="hljs-built_in">Dict</span>(
         :ex0 =&gt; sample_M_non_distributed,
         :ex1 =&gt; sample_M_tasks,
         :ex2 =&gt; sample_M_distributed,
         :ex3 =&gt; sample_M_distributed_pmap,
     )

     M = function_mapping[method](N)

     est_pi = <span class="hljs-number">4</span> * M / N

    <span class="hljs-keyword">return</span> est_pi, abs(<span class="hljs-literal">pi</span> - est_pi)
<span class="hljs-keyword">end</span></code></pre>
<p></div>
<p>For comparison, here are our final results with 4 workers:</p>
<pre><code class="julia-repl hljs">$ julia -p 4 --project
  Activating project at `~/dlh/own_lectures/ws23_ulg_vu_ama_performance_prv`
               _
   _       _ _(_)_     |  Documentation: https://docs.julialang.org
  (_)     | (_) (_)    |
   _ _   _| |_  __ _   |  Type &quot;?&quot; for help, &quot;]?&quot; for Pkg help.
  | | | | | | |/ _` |  |
  | | |_| | | | (_| |  |  Version 1.9.4 (2023-11-14)
 _/ |\__&#x27;_|_|_|\__&#x27;_|  |  Official https://julialang.org/ release
|__/                   |

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> include(<span class="hljs-string">&quot;pi_example_distributed.jl&quot;</span>)  <span class="hljs-comment"># We stored our solution code here.</span>
</span>estimate_pi_distributed (generic function with 1 method)

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> N = <span class="hljs-number">2</span>^<span class="hljs-number">30</span>
</span>1073741824

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@btime</span> estimate_pi_distributed(N, :ex0)
</span>  2.578 s (31 allocations: 1.92 KiB)
(3.1415886618196964, 3.991770096689606e-6)

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@btime</span> estimate_pi_distributed(N, :ex1)
</span>  2.578 s (55 allocations: 3.23 KiB)
(3.141537304967642, 5.5348622151285554e-5)

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@btime</span> estimate_pi_distributed(N, :ex2)
</span>  803.336 ms (336 allocations: 14.83 KiB)
(3.141508888453245, 8.376513654795303e-5)

<span class="hljs-meta prompt_">julia&gt;</span><span class="language-julia"> <span class="hljs-meta">@btime</span> estimate_pi_distributed(N, :ex3)
</span>  680.889 ms (915 allocations: 51.08 KiB)
(3.141591504216194, 1.149373598963166e-6)</code></pre>
<h1 id="additional_information_on_distributed_computing"><a href="#additional_information_on_distributed_computing" class="header-anchor">Additional information on distributed computing</a></h1>
<p>There is a lot more to say about distributed computing. Have a read in the <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/">docs</a> but here are some things we want to mention. For example, Julia is able to define <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/#man-shared-arrays">shared arrays</a>. A shared array means the content can be accessed by each worker and it is consistent over all workers.</p>
<p>Furthermore, it is possible to have a cluster, e.g. a managed pool of workers. This cluster can be distributed on several machines. We can define various ways of accessing these machines so we can become really flexible about this and maybe start a worker on the laptop of a colleague. For this we have the <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/#ClusterManagers"><code>ClusterManager</code></a> package.</p>
<div class="page-foot">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> - <a href="https://ehrensperger.dev/">Gregor Ehrensperger</a>, <a href="https://orcid.org/0000-0003-3601-0852">Peter Kandolf</a>. Last modified: January 11, 2024.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    </div> <!-- end of class main-content -->
    </div> <!-- end of class main-content-wrap -->
    </div> <!-- end of class page-wrap-->
    
      



    
    
      


      <script>
    (function(){
    
      // Get the elements.
      // - the 'pre' element.
      // - the 'div' with the 'paste-content' id.
    
      var pre = document.getElementsByTagName('pre');
    
      // Add a copy button in the 'pre' element.
      // which only has the className of 'language-'.
    
      for (var i = 0; i < pre.length; i++) {
        var isLanguage = pre[i].children[0].tagName == 'CODE';
    
        if ( isLanguage ) {
          var button           = document.createElement('button');
              button.className = 'copy-button';
              button.textContent = 'Copy';
    
              pre[i].appendChild(button);
        }
      };
    
      // Run Clipboard
    
      var copyCode = new Clipboard('.copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
    
      // On success:
      // - Change the "Copy" text to "Copied".
      // - Swap it to "Copy" in 2s.
      // - Lead user to the "contenteditable" area with Velocity scroll.
    
      copyCode.on('success', function(event) {
        event.clearSelection();
        event.trigger.textContent = 'Copied';
        window.setTimeout(function() {
          event.trigger.textContent = 'Copy';
        }, 2000);
    
      });
    
      // On error (Safari):
      // - Change the  "Press Ctrl+C to copy"
      // - Swap it to "Copy" in 2s.
    
      copyCode.on('error', function(event) {
        event.trigger.textContent = 'Press "Ctrl + C" to copy';
        window.setTimeout(function() {
          event.trigger.textContent = 'Copy';
        }, 5000);
      });
    
    })();
</script>
    
  </body>
</html>

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;

  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = this.nextElementSibling;
      if (content.style.display === "block") {
        content.style.display = "none";
      } else {
        content.style.display = "block";
      }
    });
  }
</script>

<script>
  var coll = document.getElementsByClassName("solutioncollapsible");
  var sols = document.getElementsByClassName("solution");
  
  const queryString = window.location.search;
  const urlParams = new URLSearchParams(queryString);
  const myVar = urlParams.get('solution')

  if ( myVar == 'true') {
    for (i = 0; i < coll.length; i++) {
      coll[i].style.display = "block";
    }

    for (i = 0; i < sols.length; i++) {
      sols[i].style.display = "block";
    }
  }
</script>